{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f144b3-28b0-4445-b266-87ea421d8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091a793-3496-4ec9-9dc6-5003f4b911ef",
   "metadata": {
    "panel-layout": {
     "height": 60.575,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# WEBCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bb0055-1f3f-4350-a972-76b48b0bb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    source = \"data/cake.mp4\"\n",
    "    camera = cv2.VideoCapture(source)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the Main function to start the program\n",
    "if __name__ == \"__main__\":\n",
    "    Main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b4ffe7-b11b-497d-a04c-01333a5237ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"data/cake.mp4\"\n",
    "\n",
    "if __name__ == '__name__':\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9300b-7231-4708-ba93-09c1298476a4",
   "metadata": {
    "panel-layout": {
     "height": 60.575,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Face Detection with Webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96fdac-d550-4660-af47-39c45dabbc99",
   "metadata": {
    "panel-layout": {
     "height": 60.575,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f964d10c-3932-4121-bd1d-cdd9256fb373",
   "metadata": {
    "panel-layout": {
     "height": 0,
     "visible": true,
     "width": 100
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [1,3,384,672]\n",
      "Output shape: [1,1,200,7]\n",
      "Input shape: [1,3,64,64]\n",
      "Output shape: [1,5,1,1]\n",
      "Input shape: [1,3,62,62]\n",
      "Output shape: [1,2,1,1]\n",
      "Input shape: [1,3,62,62]\n",
      "Output shape: <bound method PyCapsule.output of <CompiledModel:\n",
      "inputs[\n",
      "<ConstOutput: names[data] shape[1,3,62,62] type: f32>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[prob] shape[1,2,1,1] type: f32>,\n",
      "<ConstOutput: names[age_conv3, fc3_a] shape[1,1,1,1] type: f32>\n",
      "]>>\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "model_face = core.read_model(model='Model/face-detection-adas-0001.xml')\n",
    "compiled_model_face = core.compile_model(model = model_face, device_name= \"CPU\")\n",
    "\n",
    "input_layer_face = compiled_model_face.input(0)\n",
    "output_layer_face = compiled_model_face.output(0)\n",
    "\n",
    "print(\"Input shape:\", input_layer_face.shape)\n",
    "print(\"Output shape:\", output_layer_face.shape)\n",
    "\n",
    "model_emo = core.read_model(model='Model/emotions-recognition-retail-0003.xml')\n",
    "compiled_model_emo = core.compile_model(model = model_emo, device_name= \"CPU\")\n",
    "\n",
    "input_layer_emo = compiled_model_emo.input(0)\n",
    "output_layer_emo = compiled_model_emo.output(0)\n",
    "\n",
    "print(\"Input shape:\", input_layer_emo.shape)\n",
    "print(\"Output shape:\", output_layer_emo.shape)\n",
    "\n",
    "model_ag = core.read_model(model='Model/age-gender-recognition-retail-0013.xml')\n",
    "compiled_model_ag = core.compile_model(model = model_ag, device_name= \"CPU\")\n",
    "\n",
    "input_layer_ag = compiled_model_ag.input(0)\n",
    "output_layer_ag = compiled_model_ag.output(0)\n",
    "\n",
    "print(\"Input shape:\", input_layer_ag.shape)\n",
    "print(\"Output shape:\", output_layer_ag.shape)\n",
    "compiled_model_ag = core.compile_model(model = model_ag, device_name= \"CPU\")\n",
    "\n",
    "input_layer_ag = compiled_model_ag.input(0)\n",
    "output_layer_ag = compiled_model_ag.output\n",
    "\n",
    "print(\"Input shape:\", input_layer_ag.shape)\n",
    "print(\"Output shape:\", output_layer_ag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a9dd9-be56-48b4-b103-09a8c7c02909",
   "metadata": {
    "panel-layout": {
     "height": 44.075,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Pre-process New Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bfb6c9-aa8d-44dc-ba65-2cd792c03df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(frame, input_layer):\n",
    "    N,input_channels,input_height, input_width = input_layer.shape\n",
    "    \n",
    "    resized_frame = cv2.resize(frame, (input_width, input_height))\n",
    "    transposed_frame = resized_frame.transpose(2, 0, 1)\n",
    "    input_frame = np.expand_dims(transposed_frame, 0 )\n",
    "    \n",
    "    return input_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1f643-150b-4921-96c4-95458a4e778c",
   "metadata": {
    "panel-layout": {
     "height": 44.075,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Postprocess the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336e991-a416-45c0-9006-6154fd1791d9",
   "metadata": {
    "panel-layout": {
     "height": 44.075,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Find the face boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9927d0f-19ec-43a3-b929-50292cd6d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faceboxes(frame, results, confidence_threshold):\n",
    "    results = results.squeeze()\n",
    "\n",
    "    scores = results[:,2]\n",
    "    boxes = results[:, -4:]\n",
    "\n",
    "    face_boxes = boxes[scores >= confidence_threshold]\n",
    "    scores = scores[scores >= confidence_threshold]\n",
    "\n",
    "    frame_h, frame_w, frame_channels = frame.shape\n",
    "    face_boxes = face_boxes*np.array([frame_w, frame_h, frame_w, frame_h])\n",
    "    face_boxes = face_boxes.astype(np.int64)\n",
    "\n",
    "    return face_boxes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f93ff-5201-4f1a-8ee4-cb2cfc51839f",
   "metadata": {
    "panel-layout": {
     "height": 44.075,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Draw emotion/age/gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567ad3e7-9d2d-4bc4-995c-a1b9cee325b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_age_gender_emotion(face_boxes, frame):\n",
    "\n",
    "    EMOTION_NAMES = ['neutral', 'happy', 'sad', 'surprise', 'anger'] \n",
    "\n",
    "    show_frame = frame.copy()\n",
    "\n",
    "    for i in range(len(face_boxes)):\n",
    "\n",
    "        xmin, ymin, xmax, ymax = face_boxes[i]\n",
    "        face = frame[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        # --- emotion ---\n",
    "        input_frame = preprocess(face, input_layer_emo)\n",
    "        results_emo = compiled_model_emo([input_frame])[output_layer_emo]\n",
    "\n",
    "        results_emo.squeeze()\n",
    "        index = np.argmax(results_emo)\n",
    "\n",
    "        text = EMOTION_NAMES[index]\n",
    "        # --- emotion ---\n",
    "\n",
    "        # --- age and gender ---\n",
    "        input_frame_ag = preprocess(face, input_layer_ag)\n",
    "        results_ag = compiled_model_ag([input_frame_ag])\n",
    "        age, gender = results_ag[1], results_ag[0]\n",
    "        age = np.squeeze(age)\n",
    "        age = int(age * 100)\n",
    "\n",
    "        gender = np.squeeze(gender)\n",
    "\n",
    "        if gender[0] >= 0.65:\n",
    "            gender = \"female\"\n",
    "            box_color = (200, 200, 0)\n",
    "        elif gender[1] >= 0.55:\n",
    "            gender = \"male\"\n",
    "            box_color = (0, 200, 200)\n",
    "        else:\n",
    "            gender = \"unknown\"\n",
    "            box_color = (200, 200, 200)\n",
    "        \n",
    "        if age < 30:\n",
    "            box_color = (255, 0, 0)\n",
    "        # --- age and gender ---\n",
    "\n",
    "        fontScale = frame.shape[1] / 750\n",
    "        \n",
    "        text = gender + ' ' + str(age) + ' ' + EMOTION_NAMES[index]\n",
    "        cv2.putText(show_frame, text, (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (0, 200, 0), 8)\n",
    "        cv2.rectangle(img=show_frame, pt1=(xmin, ymin), pt2=(xmax, ymax), color=box_color, thickness=2)\n",
    "\n",
    "    return show_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d142b9d-d98b-455f-bd13-4694e17136b8",
   "metadata": {
    "panel-layout": {
     "height": 60.575,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34c21d96-3f38-49c5-87e3-ab3656ca1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    source = \"data/cake.mp4\"\n",
    "    camera = cv2.VideoCapture(source)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        input_frame = preprocess(frame, input_layer_face)\n",
    "        results = compiled_model_face([input_frame])[output_layer_face]\n",
    "\n",
    "        face_boxes, scores = find_faceboxes(frame, results, confidence_threshold)\n",
    "        show_frame = draw_age_gender_emotion(face_boxes, frame)\n",
    "\n",
    "        cv2.imshow(\"Webcam\", show_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "confidence_threshold = .95\n",
    "if __name__ == \"__main__\":\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "698aed31-ace9-4f74-ad94-ae7a6edb34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = .95\n",
    "source = \"data/cake.mp4\"\n",
    "\n",
    "if __name__ == '__name__':\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a73c7a-1ed6-47d5-9d7f-ab6b3cbc964f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8ac9a-1748-4384-aef9-0eb04cab4b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "panel-cell-order": [
   "3091a793-3496-4ec9-9dc6-5003f4b911ef",
   "61e9300b-7231-4708-ba93-09c1298476a4",
   "3b96fdac-d550-4660-af47-39c45dabbc99",
   "f964d10c-3932-4121-bd1d-cdd9256fb373",
   "950a9dd9-be56-48b4-b103-09a8c7c02909",
   "cbe1f643-150b-4921-96c4-95458a4e778c",
   "d336e991-a416-45c0-9006-6154fd1791d9",
   "b84f93ff-5201-4f1a-8ee4-cb2cfc51839f",
   "6d142b9d-d98b-455f-bd13-4694e17136b8"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
